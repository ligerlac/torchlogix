

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torchlogix.compiled_model &mdash; TorchLogix 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=01f34227"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=30646c52"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            TorchLogix
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/quickstart.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/logic_gates.html">Logic Gates in TorchLogix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/torchlogix.html">TorchLogix Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/layers.html">Layers Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/models.html">Models Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/functional.html">Functional Module</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/contributing.html">Contributing to TorchLogix</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TorchLogix</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">torchlogix.compiled_model</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for torchlogix.compiled_model</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">ctypes</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy.typing</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.layers.conv</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogicConv2d</span><span class="p">,</span> <span class="n">LogicConv3d</span><span class="p">,</span> <span class="n">OrPooling</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.layers.dense</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogicDense</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.layers.groupsum</span><span class="w"> </span><span class="kn">import</span> <span class="n">GroupSum</span>

<span class="n">ALL_OPERATIONS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;zero&quot;</span><span class="p">,</span> <span class="s2">&quot;and&quot;</span><span class="p">,</span> <span class="s2">&quot;not_implies&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;not_implied_by&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;xor&quot;</span><span class="p">,</span> <span class="s2">&quot;or&quot;</span><span class="p">,</span>
    <span class="s2">&quot;not_or&quot;</span><span class="p">,</span> <span class="s2">&quot;not_xor&quot;</span><span class="p">,</span> <span class="s2">&quot;not_b&quot;</span><span class="p">,</span> <span class="s2">&quot;implied_by&quot;</span><span class="p">,</span> <span class="s2">&quot;not_a&quot;</span><span class="p">,</span> <span class="s2">&quot;implies&quot;</span><span class="p">,</span> <span class="s2">&quot;not_and&quot;</span><span class="p">,</span> <span class="s2">&quot;one&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">BITS_TO_DTYPE</span> <span class="o">=</span> <span class="p">{</span><span class="mi">8</span><span class="p">:</span> <span class="s2">&quot;char&quot;</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="s2">&quot;short&quot;</span><span class="p">,</span> <span class="mi">32</span><span class="p">:</span> <span class="s2">&quot;int&quot;</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="s2">&quot;long long&quot;</span><span class="p">}</span>
<span class="n">BITS_TO_ZERO_LITERAL</span> <span class="o">=</span> <span class="p">{</span><span class="mi">8</span><span class="p">:</span> <span class="s2">&quot;(char) 0&quot;</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="s2">&quot;(short) 0&quot;</span><span class="p">,</span> <span class="mi">32</span><span class="p">:</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="s2">&quot;0LL&quot;</span><span class="p">}</span>
<span class="n">BITS_TO_ONE_LITERAL</span> <span class="o">=</span> <span class="p">{</span><span class="mi">8</span><span class="p">:</span> <span class="s2">&quot;(char) 1&quot;</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="s2">&quot;(short) 1&quot;</span><span class="p">,</span> <span class="mi">32</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="s2">&quot;1LL&quot;</span><span class="p">}</span>
<span class="n">BITS_TO_C_DTYPE</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">8</span><span class="p">:</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int8</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int16</span><span class="p">,</span> <span class="mi">32</span><span class="p">:</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int32</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_int64</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">BITS_TO_NP_DTYPE</span> <span class="o">=</span> <span class="p">{</span><span class="mi">8</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="mi">16</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="mi">32</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">}</span>


<div class="viewcode-block" id="CompiledLogicNet">
<a class="viewcode-back" href="../../api/torchlogix.html#torchlogix.CompiledLogicNet">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CompiledLogicNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unified compiled logic network that handles convolutional, pooling, and linear layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="CompiledLogicNet.__init__">
<a class="viewcode-back" href="../../api/torchlogix.html#torchlogix.CompiledLogicNet.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">num_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">cpu_compiler</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;gcc&quot;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span> <span class="o">=</span> <span class="n">num_bits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cpu_compiler</span> <span class="o">=</span> <span class="n">cpu_compiler</span>

        <span class="k">assert</span> <span class="n">cpu_compiler</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;clang&quot;</span><span class="p">,</span> <span class="s2">&quot;gcc&quot;</span><span class="p">],</span> <span class="n">cpu_compiler</span>
        <span class="k">assert</span> <span class="n">num_bits</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>

        <span class="c1"># Initialize layer storage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lib_fn</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_parse_model</span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_parse_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Parse the model structure, handling conv, pooling, and linear layers.&quot;&quot;&quot;</span>
        <span class="c1"># Find GroupSum layer for num_classes</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">GroupSum</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">k</span>
                <span class="k">break</span>

        <span class="c1"># Parse all layers and track execution order</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">LogicConv2d</span><span class="p">):</span>
                <span class="n">conv_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_conv_layer_info</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv_info</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">in_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layer</span><span class="o">.</span><span class="n">in_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">LogicConv3d</span><span class="p">):</span>
                <span class="n">conv_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_conv_layer_info</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv_info</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">in_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layer</span><span class="o">.</span><span class="n">in_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer</span><span class="o">.</span><span class="n">in_dim</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">OrPooling</span><span class="p">):</span>
                <span class="n">pool_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_pooling_layer_info</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pool_info</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;pool&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pooling_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">LogicDense</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layer</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s1">&#39;flatten&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found Flatten layer&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">GroupSum</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found GroupSum layer with </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">k</span><span class="si">}</span><span class="s2"> classes&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Unknown layer type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parsed </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">)</span><span class="si">}</span><span class="s2"> conv, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pooling_layers</span><span class="p">)</span><span class="si">}</span><span class="s2"> pooling, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">)</span><span class="si">}</span><span class="s2"> linear layers&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer execution order: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Validate model structure</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Model must contain at least one LogicConv2d, LogicConv3d, or LogicDense layer.&quot;</span><span class="p">)</span>

        <span class="c1"># Set input shape for linear-only models</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">:</span>
            <span class="n">first_linear</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">LogicDense</span><span class="p">):</span>
                    <span class="n">first_linear</span> <span class="o">=</span> <span class="n">layer</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">first_linear</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">first_linear</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extract_conv_layer_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">LogicConv2d</span><span class="p">,</span> <span class="n">LogicConv3d</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract information from a LogicConv2d or LogicConv3d layer for compilation.&quot;&quot;&quot;</span>
        <span class="n">tree_operations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">level_idx</span><span class="p">,</span> <span class="n">level_weights</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">tree_weights</span><span class="p">):</span>
            <span class="n">level_ops</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">weight_param</span> <span class="ow">in</span> <span class="n">level_weights</span><span class="p">:</span>
                <span class="n">ops</span> <span class="o">=</span> <span class="n">weight_param</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="n">level_ops</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ops</span><span class="p">)</span>
            <span class="n">tree_operations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">level_ops</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;indices&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
            <span class="s1">&#39;tree_operations&#39;</span><span class="p">:</span> <span class="n">tree_operations</span><span class="p">,</span>
            <span class="s1">&#39;tree_depth&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">tree_depth</span><span class="p">,</span>
            <span class="s1">&#39;num_kernels&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">num_kernels</span><span class="p">,</span>
            <span class="s1">&#39;in_dim&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">in_dim</span><span class="p">,</span>
            <span class="s1">&#39;receptive_field_size&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">receptive_field_size</span><span class="p">,</span>
            <span class="s1">&#39;stride&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="s1">&#39;padding&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="s1">&#39;channels&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extract_pooling_layer_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="n">OrPooling</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract information from an OrPooling layer for compilation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;kernel_size&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="s1">&#39;stride&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="s1">&#39;padding&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
        <span class="p">}</span>

<div class="viewcode-block" id="CompiledLogicNet.get_gate_code">
<a class="viewcode-back" href="../../api/torchlogix.html#torchlogix.CompiledLogicNet.get_gate_code">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_gate_code</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">var2</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">gate_op</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate C code for a logic gate operation.&quot;&quot;&quot;</span>
        <span class="n">operation_name</span> <span class="o">=</span> <span class="n">ALL_OPERATIONS</span><span class="p">[</span><span class="n">gate_op</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;zero&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">BITS_TO_ZERO_LITERAL</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;and&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> &amp; </span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;not_implies&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> &amp; ~</span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;a&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;not_implied_by&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2"> &amp; ~</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;xor&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> ^ </span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;or&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;not_or&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~(</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;not_xor&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~(</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> ^ </span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;not_b&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~</span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;implied_by&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~</span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;not_a&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;implies&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;not_and&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~(</span><span class="si">{</span><span class="n">var1</span><span class="si">}</span><span class="s2"> &amp; </span><span class="si">{</span><span class="n">var2</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="k">elif</span> <span class="n">operation_name</span> <span class="o">==</span> <span class="s2">&quot;one&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;~</span><span class="si">{</span><span class="n">BITS_TO_ZERO_LITERAL</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Operator </span><span class="si">{</span><span class="n">operation_name</span><span class="si">}</span><span class="s2"> unknown.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(char) (</span><span class="si">{</span><span class="n">res</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(short) (</span><span class="si">{</span><span class="n">res</span><span class="si">}</span><span class="s2">)&quot;</span>

        <span class="k">return</span> <span class="n">res</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_calculate_layer_output_sizes_and_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">tuple</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate output sizes and shapes for all layers in execution order.&quot;&quot;&quot;</span>
        <span class="n">layer_info</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span>

        <span class="k">for</span> <span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_order</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>
                <span class="n">conv_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># (C, H, W)</span>
                    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">current_shape</span>
                    <span class="n">h_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">h</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">w_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">w</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;num_kernels&#39;</span><span class="p">],</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">)</span>
                    <span class="n">output_size</span> <span class="o">=</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;num_kernels&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">h_out</span> <span class="o">*</span> <span class="n">w_out</span>
                <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span> <span class="c1"># (C, H, W, D)</span>
                    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">current_shape</span>
                    <span class="n">h_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">h</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">w_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">w</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">d_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;num_kernels&#39;</span><span class="p">],</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
                    <span class="n">output_size</span> <span class="o">=</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;num_kernels&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">h_out</span> <span class="o">*</span> <span class="n">w_out</span> <span class="o">*</span> <span class="n">d_out</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conv layer expects 3D or 4D input, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;pool&#39;</span><span class="p">:</span>
                <span class="n">pool_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># (C, H, W)</span>
                    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">current_shape</span>
                    <span class="n">h_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">h</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">])</span> 
                             <span class="o">//</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">w_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">w</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">])</span> 
                             <span class="o">//</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">)</span>
                    <span class="n">output_size</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">h_out</span> <span class="o">*</span> <span class="n">w_out</span>
                <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span> <span class="c1"># (C, H, W, D)</span>
                    <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">current_shape</span>
                    <span class="n">h_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">h</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">w_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">w</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">d_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">])</span>
                             <span class="o">//</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
                    <span class="n">output_size</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">h_out</span> <span class="o">*</span> <span class="n">w_out</span> <span class="o">*</span> <span class="n">d_out</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pool layer expects 3D or 4D input, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">):</span>
                    <span class="n">output_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">current_shape</span><span class="p">)</span>
                    <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output_size</span> <span class="o">=</span> <span class="n">current_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">output_shape</span> <span class="o">=</span> <span class="n">current_shape</span>

            <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
                <span class="n">layer_a</span><span class="p">,</span> <span class="n">layer_b</span><span class="p">,</span> <span class="n">layer_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
                <span class="n">output_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_a</span><span class="p">)</span>
                <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,)</span>

            <span class="n">layer_info</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_size</span><span class="p">))</span>
            <span class="n">current_shape</span> <span class="o">=</span> <span class="n">output_shape</span>

        <span class="k">return</span> <span class="n">layer_info</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_conv_layer_code</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conv_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate C code for a convolutional layer.&quot;&quot;&quot;</span>
        <span class="n">code</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">]</span>
        <span class="n">tree_ops</span> <span class="o">=</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;tree_operations&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">conv_dim</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">])</span>
                    <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">])</span>
                    <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">conv_dim</span> <span class="o">=</span> <span class="mi">3</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
                    <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                    <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="p">((</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;receptive_field_size&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
                    <span class="o">//</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conv layer expects 3D or 4D input, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Convolutional layer </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">kernel_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;num_kernels&#39;</span><span class="p">]):</span>
            <span class="n">iter_range</span> <span class="o">=</span> <span class="n">h_out</span> <span class="o">*</span> <span class="n">w_out</span>
            <span class="k">if</span> <span class="n">conv_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">iter_range</span> <span class="o">*=</span> <span class="n">d_out</span>
            <span class="k">for</span> <span class="n">pos_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iter_range</span><span class="p">):</span>
                <span class="c1"># First level: process receptive field positions</span>
                <span class="n">level_0_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">left_indices</span> <span class="o">=</span> <span class="n">level_0_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">kernel_idx</span><span class="p">,</span> <span class="n">pos_idx</span><span class="p">]</span>
                <span class="n">right_indices</span> <span class="o">=</span> <span class="n">level_0_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">kernel_idx</span><span class="p">,</span> <span class="n">pos_idx</span><span class="p">]</span>

                <span class="c1"># Generate variables for the first level</span>
                <span class="k">for</span> <span class="n">gate_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;tree_depth&#39;</span><span class="p">]):</span>
                    <span class="k">if</span> <span class="n">conv_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                        <span class="n">left_h</span><span class="p">,</span> <span class="n">left_w</span><span class="p">,</span> <span class="n">left_c</span> <span class="o">=</span> <span class="n">left_indices</span><span class="p">[</span><span class="n">gate_idx</span><span class="p">]</span>
                        <span class="n">right_h</span><span class="p">,</span> <span class="n">right_w</span><span class="p">,</span> <span class="n">right_c</span> <span class="o">=</span> <span class="n">right_indices</span><span class="p">[</span><span class="n">gate_idx</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">left_h</span><span class="p">,</span> <span class="n">left_w</span><span class="p">,</span> <span class="n">left_d</span><span class="p">,</span> <span class="n">left_c</span> <span class="o">=</span> <span class="n">left_indices</span><span class="p">[</span><span class="n">gate_idx</span><span class="p">]</span>
                        <span class="n">right_h</span><span class="p">,</span> <span class="n">right_w</span><span class="p">,</span> <span class="n">right_d</span><span class="p">,</span> <span class="n">right_c</span> <span class="o">=</span> <span class="n">right_indices</span><span class="p">[</span><span class="n">gate_idx</span><span class="p">]</span>

                    <span class="n">gate_op</span> <span class="o">=</span> <span class="n">tree_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">gate_idx</span><span class="p">][</span><span class="n">kernel_idx</span><span class="p">]</span>

                    <span class="c1"># Determine input source</span>
                    <span class="n">prev_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_previous_layer_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">prev_layer_name</span> <span class="o">==</span> <span class="s2">&quot;inp&quot;</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">conv_dim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
                            <span class="n">left_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;inp[</span><span class="si">{</span><span class="n">left_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">left_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">left_w</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>
                            <span class="n">right_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;inp[</span><span class="si">{</span><span class="n">right_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">right_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">right_w</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">left_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;inp[</span><span class="si">{</span><span class="n">left_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">left_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">left_w</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">left_d</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>
                            <span class="n">right_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;inp[</span><span class="si">{</span><span class="n">right_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">right_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">right_w</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">right_d</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">conv_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                            <span class="n">left_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">left_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">left_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">left_w</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>
                            <span class="n">right_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">right_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">right_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">right_w</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">left_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">left_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">left_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">left_w</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">left_d</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>
                            <span class="n">right_var</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">right_c</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">right_h</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">right_w</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;in_dim&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">right_d</span><span class="si">}</span><span class="s2">]&quot;</span>
                            <span class="p">)</span>


                    <span class="n">var_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;conv_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_k</span><span class="si">{</span><span class="n">kernel_idx</span><span class="si">}</span><span class="s2">_p</span><span class="si">{</span><span class="n">pos_idx</span><span class="si">}</span><span class="s2">_l0_g</span><span class="si">{</span><span class="n">gate_idx</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">const </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">var_name</span><span class="si">}</span><span class="s2"> = &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">get_gate_code</span><span class="p">(</span><span class="n">left_var</span><span class="p">,</span><span class="w"> </span><span class="n">right_var</span><span class="p">,</span><span class="w"> </span><span class="n">gate_op</span><span class="p">)</span><span class="si">}</span><span class="s2">;&quot;</span>
                    <span class="p">)</span>

                <span class="c1"># Process remaining tree levels</span>
                <span class="k">for</span> <span class="n">level</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;tree_depth&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">level_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">level</span><span class="p">]</span>
                    <span class="n">left_gate_indices</span> <span class="o">=</span> <span class="n">level_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">right_gate_indices</span> <span class="o">=</span> <span class="n">level_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

                    <span class="n">num_gates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">left_gate_indices</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">gate_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gates</span><span class="p">):</span>
                        <span class="n">left_idx</span> <span class="o">=</span> <span class="n">left_gate_indices</span><span class="p">[</span><span class="n">gate_idx</span><span class="p">]</span>
                        <span class="n">right_idx</span> <span class="o">=</span> <span class="n">right_gate_indices</span><span class="p">[</span><span class="n">gate_idx</span><span class="p">]</span>

                        <span class="n">gate_op</span> <span class="o">=</span> <span class="n">tree_ops</span><span class="p">[</span><span class="n">level</span><span class="p">][</span><span class="n">gate_idx</span><span class="p">][</span><span class="n">kernel_idx</span><span class="p">]</span>

                        <span class="n">left_var</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;conv_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_k</span><span class="si">{</span><span class="n">kernel_idx</span><span class="si">}</span><span class="s2">_p</span><span class="si">{</span><span class="n">pos_idx</span><span class="si">}</span><span class="s2">_l</span><span class="si">{</span><span class="n">level</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s2">_g</span><span class="si">{</span><span class="n">left_idx</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="n">right_var</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;conv_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_k</span><span class="si">{</span><span class="n">kernel_idx</span><span class="si">}</span><span class="s2">_p</span><span class="si">{</span><span class="n">pos_idx</span><span class="si">}</span><span class="s2">_l</span><span class="si">{</span><span class="n">level</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s2">_g</span><span class="si">{</span><span class="n">right_idx</span><span class="si">}</span><span class="s2">&quot;</span>

                        <span class="k">if</span> <span class="n">level</span> <span class="o">==</span> <span class="n">conv_info</span><span class="p">[</span><span class="s1">&#39;tree_depth&#39;</span><span class="p">]:</span>
                            <span class="k">if</span> <span class="n">conv_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                                <span class="n">output_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_idx</span> <span class="o">*</span> <span class="n">h_out</span> <span class="o">*</span> <span class="n">w_out</span> <span class="o">+</span> <span class="n">pos_idx</span><span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">output_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_idx</span> <span class="o">*</span> <span class="n">h_out</span> <span class="o">*</span> <span class="n">w_out</span> <span class="o">*</span> <span class="n">d_out</span> <span class="o">+</span> <span class="n">pos_idx</span><span class="p">)</span>
                            <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">layer_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2">] = &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">get_gate_code</span><span class="p">(</span><span class="n">left_var</span><span class="p">,</span><span class="w"> </span><span class="n">right_var</span><span class="p">,</span><span class="w"> </span><span class="n">gate_op</span><span class="p">)</span><span class="si">}</span><span class="s2">;&quot;</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">var_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;conv_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_k</span><span class="si">{</span><span class="n">kernel_idx</span><span class="si">}</span><span class="s2">_p</span><span class="si">{</span><span class="n">pos_idx</span><span class="si">}</span><span class="s2">_l</span><span class="si">{</span><span class="n">level</span><span class="si">}</span><span class="s2">_g</span><span class="si">{</span><span class="n">gate_idx</span><span class="si">}</span><span class="s2">&quot;</span>
                            <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">const </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">var_name</span><span class="si">}</span><span class="s2"> = &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">get_gate_code</span><span class="p">(</span><span class="n">left_var</span><span class="p">,</span><span class="w"> </span><span class="n">right_var</span><span class="p">,</span><span class="w"> </span><span class="n">gate_op</span><span class="p">)</span><span class="si">}</span><span class="s2">;&quot;</span>
                            <span class="p">)</span>

        <span class="k">return</span> <span class="n">code</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_pooling_layer_code</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool_info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate C code for an OrPooling (max pooling) layer.&quot;&quot;&quot;</span>
        <span class="n">code</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">pool_dim</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="n">channels</span><span class="p">,</span> <span class="n">in_h</span><span class="p">,</span> <span class="n">in_w</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">pool_dim</span> <span class="o">=</span> <span class="mi">3</span>
            <span class="n">channels</span><span class="p">,</span> <span class="n">in_h</span><span class="p">,</span> <span class="n">in_w</span><span class="p">,</span> <span class="n">in_d</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pool layer expects 3D or 4D input, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">D&quot;</span><span class="p">)</span>

        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">]</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">]</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">pool_info</span><span class="p">[</span><span class="s1">&#39;padding&#39;</span><span class="p">]</span>

        <span class="c1"># Calculate output dimensions</span>
        <span class="n">out_h</span> <span class="o">=</span> <span class="p">((</span><span class="n">in_h</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">-</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">out_w</span> <span class="o">=</span> <span class="p">((</span><span class="n">in_w</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">-</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">pool_dim</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
            <span class="n">out_d</span> <span class="o">=</span> <span class="p">((</span><span class="n">in_d</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span> <span class="o">-</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Max pooling layer </span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pool_dim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
            <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Input: </span><span class="si">{</span><span class="n">channels</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">in_h</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">in_w</span><span class="si">}</span><span class="s2">, Output: </span><span class="si">{</span><span class="n">channels</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">out_h</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">out_w</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Input: </span><span class="si">{</span><span class="n">channels</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">in_h</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">in_w</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">in_d</span><span class="si">}</span><span class="s2">, Output: </span><span class="si">{</span><span class="n">channels</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">out_h</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">out_w</span><span class="si">}</span><span class="s2">x</span><span class="si">{</span><span class="n">out_d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Generate pooling code for each channel and output position</span>
        <span class="n">prev_layer_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_previous_layer_name</span><span class="p">(</span><span class="n">layer_name</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">pool_dim</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">out_y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_h</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">out_x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_w</span><span class="p">):</span>
                        <span class="c1"># Calculate input region for this output position</span>
                        <span class="n">in_y_start</span> <span class="o">=</span> <span class="n">out_y</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="n">padding</span>
                        <span class="n">in_x_start</span> <span class="o">=</span> <span class="n">out_x</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="n">padding</span>
                        <span class="n">in_y_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">in_y_start</span> <span class="o">+</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">in_h</span><span class="p">)</span>
                        <span class="n">in_x_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">in_x_start</span> <span class="o">+</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">in_w</span><span class="p">)</span>
                        <span class="n">in_y_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_y_start</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                        <span class="n">in_x_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_x_start</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                        <span class="n">output_idx</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">out_h</span> <span class="o">*</span> <span class="n">out_w</span> <span class="o">+</span> <span class="n">out_y</span> <span class="o">*</span> <span class="n">out_w</span> <span class="o">+</span> <span class="n">out_x</span>

                        <span class="c1"># Initialize with first valid input</span>
                        <span class="n">first_input_idx</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">in_h</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">+</span> <span class="n">in_y_start</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">+</span> <span class="n">in_x_start</span>
                        <span class="k">if</span> <span class="n">prev_layer_name</span> <span class="o">==</span> <span class="s2">&quot;inp&quot;</span><span class="p">:</span>
                            <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">layer_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2">] = inp[</span><span class="si">{</span><span class="n">first_input_idx</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">layer_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2">] = layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">first_input_idx</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>

                        <span class="c1"># Compare with remaining inputs in the kernel window (max pooling using OR)</span>
                        <span class="k">for</span> <span class="n">ky</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_y_start</span><span class="p">,</span> <span class="n">in_y_end</span><span class="p">):</span>
                            <span class="k">for</span> <span class="n">kx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_x_start</span><span class="p">,</span> <span class="n">in_x_end</span><span class="p">):</span>
                                <span class="k">if</span> <span class="n">ky</span> <span class="o">==</span> <span class="n">in_y_start</span> <span class="ow">and</span> <span class="n">kx</span> <span class="o">==</span> <span class="n">in_x_start</span><span class="p">:</span>
                                    <span class="k">continue</span>  <span class="c1"># Skip first element (already initialized)</span>

                                <span class="n">input_idx</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">in_h</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">+</span> <span class="n">ky</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">+</span> <span class="n">kx</span>
                                <span class="k">if</span> <span class="n">prev_layer_name</span> <span class="o">==</span> <span class="s2">&quot;inp&quot;</span><span class="p">:</span>
                                    <span class="n">input_var</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;inp[</span><span class="si">{</span><span class="n">input_idx</span><span class="si">}</span><span class="s2">]&quot;</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">input_var</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">input_idx</span><span class="si">}</span><span class="s2">]&quot;</span>

                                <span class="c1"># Max operation using bitwise OR (since we&#39;re working with binary values)</span>
                                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">layer_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2">] |= </span><span class="si">{</span><span class="n">input_var</span><span class="si">}</span><span class="s2">;&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">out_z</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_h</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">out_y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_w</span><span class="p">):</span>
                        <span class="k">for</span> <span class="n">out_x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">out_d</span><span class="p">):</span>
                            <span class="c1"># Calculate input region for this output position</span>
                            <span class="n">in_z_start</span> <span class="o">=</span> <span class="n">out_z</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="n">padding</span>
                            <span class="n">in_y_start</span> <span class="o">=</span> <span class="n">out_y</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="n">padding</span>
                            <span class="n">in_x_start</span> <span class="o">=</span> <span class="n">out_x</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="n">padding</span>
                            <span class="n">in_z_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">in_z_start</span> <span class="o">+</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">in_h</span><span class="p">)</span>
                            <span class="n">in_y_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">in_y_start</span> <span class="o">+</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">in_w</span><span class="p">)</span>
                            <span class="n">in_x_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">in_x_start</span> <span class="o">+</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">in_d</span><span class="p">)</span>
                            <span class="n">in_z_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_z_start</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                            <span class="n">in_y_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_y_start</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                            <span class="n">in_x_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_x_start</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

                            <span class="n">output_idx</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="n">c</span> <span class="o">*</span> <span class="n">out_h</span> <span class="o">*</span> <span class="n">out_w</span> <span class="o">*</span> <span class="n">out_d</span>
                                <span class="o">+</span> <span class="n">out_z</span> <span class="o">*</span> <span class="n">out_w</span> <span class="o">*</span> <span class="n">out_d</span>
                                <span class="o">+</span> <span class="n">out_y</span> <span class="o">*</span> <span class="n">out_d</span>
                                <span class="o">+</span> <span class="n">out_x</span>
                             <span class="p">)</span>

                            <span class="c1"># Initialize with first valid input</span>
                            <span class="n">first_input_idx</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="n">c</span> <span class="o">*</span> <span class="n">in_h</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">*</span> <span class="n">in_d</span>
                                <span class="o">+</span> <span class="n">in_z_start</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">*</span> <span class="n">in_d</span>
                                <span class="o">+</span> <span class="n">in_y_start</span> <span class="o">*</span> <span class="n">in_d</span>
                                <span class="o">+</span> <span class="n">in_x_start</span>
                            <span class="p">)</span>
                            <span class="k">if</span> <span class="n">prev_layer_name</span> <span class="o">==</span> <span class="s2">&quot;inp&quot;</span><span class="p">:</span>
                                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">layer_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2">] = inp[</span><span class="si">{</span><span class="n">first_input_idx</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">layer_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2">] = layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">first_input_idx</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>

                            <span class="c1"># Compare with remaining inputs in the kernel window (max pooling using OR)</span>
                            <span class="k">for</span> <span class="n">kz</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_z_start</span><span class="p">,</span> <span class="n">in_z_end</span><span class="p">):</span>
                                <span class="k">for</span> <span class="n">ky</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_y_start</span><span class="p">,</span> <span class="n">in_y_end</span><span class="p">):</span>
                                    <span class="k">for</span> <span class="n">kx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">in_x_start</span><span class="p">,</span> <span class="n">in_x_end</span><span class="p">):</span>
                                        <span class="k">if</span> <span class="p">(</span><span class="n">ky</span> <span class="o">==</span> <span class="n">in_y_start</span> <span class="ow">and</span> <span class="n">kx</span> <span class="o">==</span> <span class="n">in_x_start</span><span class="p">)</span> <span class="ow">and</span> <span class="n">kz</span> <span class="o">==</span> <span class="n">in_z_start</span><span class="p">:</span>
                                            <span class="k">continue</span>  <span class="c1"># Skip first element (already initialized)</span>

                                        <span class="n">input_idx</span> <span class="o">=</span> <span class="p">(</span>
                                            <span class="n">c</span> <span class="o">*</span> <span class="n">in_h</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">*</span> <span class="n">in_d</span>
                                            <span class="o">+</span> <span class="n">kz</span> <span class="o">*</span> <span class="n">in_w</span> <span class="o">*</span> <span class="n">in_d</span>
                                            <span class="o">+</span> <span class="n">ky</span> <span class="o">*</span> <span class="n">in_d</span>
                                            <span class="o">+</span> <span class="n">kx</span>
                                        <span class="p">)</span>
                                        <span class="k">if</span> <span class="n">prev_layer_name</span> <span class="o">==</span> <span class="s2">&quot;inp&quot;</span><span class="p">:</span>
                                            <span class="n">input_var</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;inp[</span><span class="si">{</span><span class="n">input_idx</span><span class="si">}</span><span class="s2">]&quot;</span>
                                        <span class="k">else</span><span class="p">:</span>
                                            <span class="n">input_var</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;layer_</span><span class="si">{</span><span class="n">prev_layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">input_idx</span><span class="si">}</span><span class="s2">]&quot;</span>

                                        <span class="c1"># Max operation using bitwise OR (since we&#39;re working with binary values)</span>
                                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">layer_</span><span class="si">{</span><span class="n">layer_name</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_idx</span><span class="si">}</span><span class="s2">] |= </span><span class="si">{</span><span class="n">input_var</span><span class="si">}</span><span class="s2">;&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">code</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_previous_layer_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_layer_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the name of the previous layer in execution order.&quot;&quot;&quot;</span>
        <span class="n">layer_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_layer_output_sizes_and_shapes</span><span class="p">()</span>

        <span class="c1"># Find current layer in execution order</span>
        <span class="n">current_idx</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer_info</span><span class="p">):</span>
            <span class="k">if</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s2">&quot;</span> <span class="o">==</span> <span class="n">current_layer_name</span><span class="p">:</span>
                <span class="n">current_idx</span> <span class="o">=</span> <span class="n">i</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">current_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">current_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;inp&quot;</span>  <span class="c1"># First layer uses input</span>

        <span class="c1"># Get previous layer info</span>
        <span class="n">prev_layer_type</span><span class="p">,</span> <span class="n">prev_layer_idx</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">layer_info</span><span class="p">[</span><span class="n">current_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">prev_layer_type</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span><span class="p">:</span>
            <span class="c1"># Flatten doesn&#39;t create output, so go one more layer back</span>
            <span class="k">if</span> <span class="n">current_idx</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">prev_prev_layer_type</span><span class="p">,</span> <span class="n">prev_prev_layer_idx</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">layer_info</span><span class="p">[</span><span class="n">current_idx</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
                <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prev_prev_layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">prev_prev_layer_idx</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="s2">&quot;inp&quot;</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prev_layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">prev_layer_idx</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_linear_layer_code</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_a</span><span class="p">,</span> <span class="n">layer_b</span><span class="p">,</span> <span class="n">layer_op</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">is_final</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">has_non_linear_layers</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate C code for a linear layer.&quot;&quot;&quot;</span>
        <span class="n">code</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">has_flatten</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">lt</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span> <span class="k">for</span> <span class="n">lt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_layer_output_sizes_and_shapes</span><span class="p">())</span>

        <span class="c1"># Determine input source based on layer position and model structure</span>
        <span class="k">if</span> <span class="n">has_non_linear_layers</span> <span class="ow">or</span> <span class="n">has_flatten</span><span class="p">:</span>
            <span class="c1"># Mixed model or model with flatten: first layer uses linear_input, subsequent layers alternate</span>
            <span class="k">if</span> <span class="n">layer_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">input_var</span> <span class="o">=</span> <span class="s2">&quot;linear_input&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Alternate between linear_input and linear_buf_temp</span>
                <span class="k">if</span> <span class="n">layer_id</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">input_var</span> <span class="o">=</span> <span class="s2">&quot;linear_buf_temp&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">input_var</span> <span class="o">=</span> <span class="s2">&quot;linear_input&quot;</span>
        <span class="k">elif</span> <span class="n">layer_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># First layer in linear-only model: use direct input</span>
            <span class="n">input_var</span> <span class="o">=</span> <span class="s2">&quot;inp&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Subsequent layers in linear-only model: alternate between buffers</span>
            <span class="k">if</span> <span class="n">layer_id</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">input_var</span> <span class="o">=</span> <span class="s2">&quot;linear_buf_a&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_var</span> <span class="o">=</span> <span class="s2">&quot;linear_buf_b&quot;</span>

        <span class="c1"># Determine output destination</span>
        <span class="k">if</span> <span class="n">is_final</span><span class="p">:</span>
            <span class="n">output_var</span> <span class="o">=</span> <span class="s2">&quot;out&quot;</span>
        <span class="k">elif</span> <span class="n">has_non_linear_layers</span> <span class="ow">or</span> <span class="n">has_flatten</span><span class="p">:</span>
            <span class="c1"># Mixed model or flatten model: alternate between buffers</span>
            <span class="k">if</span> <span class="n">layer_id</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">output_var</span> <span class="o">=</span> <span class="s2">&quot;linear_buf_temp&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_var</span> <span class="o">=</span> <span class="s2">&quot;linear_input&quot;</span>
        <span class="k">elif</span> <span class="n">layer_id</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Multi-layer linear model: alternate between buffers</span>
            <span class="k">if</span> <span class="n">layer_id</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">output_var</span> <span class="o">=</span> <span class="s2">&quot;linear_buf_a&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output_var</span> <span class="o">=</span> <span class="s2">&quot;linear_buf_b&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_var</span> <span class="o">=</span> <span class="s2">&quot;out&quot;</span>

        <span class="k">for</span> <span class="n">var_id</span><span class="p">,</span> <span class="p">(</span><span class="n">gate_a</span><span class="p">,</span> <span class="n">gate_b</span><span class="p">,</span> <span class="n">gate_op</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">layer_a</span><span class="p">,</span> <span class="n">layer_b</span><span class="p">,</span> <span class="n">layer_op</span><span class="p">)):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">input_var</span><span class="si">}</span><span class="s2">[</span><span class="si">{</span><span class="n">gate_a</span><span class="si">}</span><span class="s2">]&quot;</span>
            <span class="n">b</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">input_var</span><span class="si">}</span><span class="s2">[</span><span class="si">{</span><span class="n">gate_b</span><span class="si">}</span><span class="s2">]&quot;</span>
            <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">output_var</span><span class="si">}</span><span class="s2">[</span><span class="si">{</span><span class="n">var_id</span><span class="si">}</span><span class="s2">] = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">get_gate_code</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">gate_op</span><span class="p">)</span><span class="si">}</span><span class="s2">;&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">code</span>

<div class="viewcode-block" id="CompiledLogicNet.get_c_code">
<a class="viewcode-back" href="../../api/torchlogix.html#torchlogix.CompiledLogicNet.get_c_code">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_c_code</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate the complete C code for the network.&quot;&quot;&quot;</span>
        <span class="n">code</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;#include &lt;stddef.h&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;#include &lt;stdlib.h&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;#include &lt;stdbool.h&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;#include &lt;string.h&gt;&quot;</span><span class="p">,</span>
            <span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;void logic_net(&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> const *inp, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> *out);&quot;</span><span class="p">,</span>
            <span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="sa">f</span><span class="s2">&quot;void logic_net(&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> const *inp, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> *out) </span><span class="se">{{</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="p">]</span>

        <span class="c1"># Calculate sizes and shapes for all layers</span>
        <span class="n">layer_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_layer_output_sizes_and_shapes</span><span class="p">()</span>

        <span class="c1"># Allocate intermediate buffers for non-linear layers</span>
        <span class="k">for</span> <span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_size</span> <span class="ow">in</span> <span class="n">layer_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">layer_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;pool&#39;</span><span class="p">]:</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> layer_</span><span class="si">{</span><span class="n">layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s2">_out[</span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>

        <span class="c1"># Allocate buffers for linear layers if needed</span>
        <span class="n">linear_layer_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">)</span>
        <span class="n">has_non_linear_layers</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">lt</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;pool&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">lt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">layer_info</span><span class="p">)</span>
        <span class="n">has_flatten</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">lt</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span> <span class="k">for</span> <span class="n">lt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">layer_info</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">linear_layer_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">has_non_linear_layers</span> <span class="ow">or</span> <span class="n">has_flatten</span><span class="p">:</span>
                <span class="c1"># Mixed model or model with flatten: need buffer to transfer data to linear layers</span>
                <span class="c1"># Find the size after flattening or from conv/pool layers</span>
                <span class="n">flatten_size</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_size</span> <span class="ow">in</span> <span class="n">layer_info</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span><span class="p">:</span>
                        <span class="n">flatten_size</span> <span class="o">=</span> <span class="n">output_size</span>
                        <span class="k">break</span>
                
                <span class="c1"># If no explicit flatten but we have conv/pool, use the last conv/pool output size</span>
                <span class="k">if</span> <span class="n">flatten_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">has_non_linear_layers</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_size</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">layer_info</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">layer_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;pool&#39;</span><span class="p">]:</span>
                            <span class="n">flatten_size</span> <span class="o">=</span> <span class="n">output_size</span>
                            <span class="k">break</span>

                <span class="k">if</span> <span class="n">flatten_size</span><span class="p">:</span>
                    <span class="c1"># For multi-layer linear networks, we need the buffer to be large enough</span>
                    <span class="c1"># for the largest intermediate layer size</span>
                    <span class="n">max_linear_input_size</span> <span class="o">=</span> <span class="n">flatten_size</span>
                    <span class="k">if</span> <span class="n">linear_layer_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="c1"># Check all linear layer input sizes</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">layer_a</span><span class="p">,</span> <span class="n">layer_b</span><span class="p">,</span> <span class="n">layer_op</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                            <span class="n">layer_output_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_a</span><span class="p">)</span>  <span class="c1"># This layer&#39;s output size</span>
                            <span class="n">max_linear_input_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_linear_input_size</span><span class="p">,</span> <span class="n">layer_output_size</span><span class="p">)</span>

                    <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> linear_input[</span><span class="si">{</span><span class="n">max_linear_input_size</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>

            <span class="c1"># For multi-layer linear networks, use ping-pong buffers</span>
            <span class="k">if</span> <span class="n">linear_layer_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">has_non_linear_layers</span> <span class="ow">or</span> <span class="n">has_flatten</span><span class="p">:</span>
                    <span class="c1"># Mixed model or flatten model: use one additional buffer for ping-ponging</span>
                    <span class="n">max_linear_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">)</span>
                    <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> linear_buf_temp[</span><span class="si">{</span><span class="n">max_linear_size</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Linear-only model: use two ping-pong buffers</span>
                    <span class="n">max_linear_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">)</span>
                    <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> linear_buf_a[</span><span class="si">{</span><span class="n">max_linear_size</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>
                    <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> linear_buf_b[</span><span class="si">{</span><span class="n">max_linear_size</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>

        <span class="c1"># Check if we need a flatten buffer when no linear layers follow (for GroupSum only)</span>
        <span class="k">if</span> <span class="n">has_flatten</span> <span class="ow">and</span> <span class="n">linear_layer_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Need buffer for flatten  GroupSum case</span>
            <span class="n">flatten_size</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">for</span> <span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_size</span> <span class="ow">in</span> <span class="n">layer_info</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span><span class="p">:</span>
                    <span class="n">flatten_size</span> <span class="o">=</span> <span class="n">output_size</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">flatten_size</span><span class="p">:</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> flattened_output[</span><span class="si">{</span><span class="n">flatten_size</span><span class="si">}</span><span class="s2">];&quot;</span><span class="p">)</span>

        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="c1"># Generate code for all layers in execution order</span>
        <span class="n">linear_layer_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">current_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span>

        <span class="k">for</span> <span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_size</span> <span class="ow">in</span> <span class="n">layer_info</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>
                <span class="n">layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">code</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_conv_layer_code</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">],</span> <span class="n">layer_name</span><span class="p">))</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;pool&#39;</span><span class="p">:</span>
                <span class="n">layer_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">code</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_pooling_layer_code</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pooling_layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">],</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">current_shape</span><span class="p">))</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span><span class="p">:</span>
                <span class="c1"># Handle flattening: copy from input or previous layer to appropriate buffer</span>
                <span class="n">current_layer_idx</span> <span class="o">=</span> <span class="n">layer_info</span><span class="o">.</span><span class="n">index</span><span class="p">((</span><span class="n">layer_type</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">output_size</span><span class="p">))</span>
                
                <span class="k">if</span> <span class="n">current_layer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># First layer is flatten: copy directly from input</span>
                    <span class="k">if</span> <span class="n">linear_layer_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Flatten layer: copy from input to linear_input&quot;</span><span class="p">)</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">memcpy(linear_input, inp, &quot;</span>
                                   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2"> * sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Flatten layer: copy from input to flattened_output&quot;</span><span class="p">)</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">memcpy(flattened_output, inp, &quot;</span>
                                   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output_size</span><span class="si">}</span><span class="s2"> * sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Flatten follows other layers: copy from previous layer</span>
                    <span class="n">prev_layer_info</span> <span class="o">=</span> <span class="n">layer_info</span><span class="p">[</span><span class="n">current_layer_idx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="n">prev_layer_type</span><span class="p">,</span> <span class="n">prev_layer_idx</span><span class="p">,</span> <span class="n">prev_shape</span><span class="p">,</span> <span class="n">prev_size</span> <span class="o">=</span> <span class="n">prev_layer_info</span>

                    <span class="k">if</span> <span class="n">linear_layer_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># Flatten  Linear case: copy to linear_input</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Flatten layer: copy from layer_</span><span class="si">{</span><span class="n">prev_layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">prev_layer_idx</span><span class="si">}</span><span class="s2">_out to linear_input&quot;</span><span class="p">)</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">memcpy(linear_input, layer_</span><span class="si">{</span><span class="n">prev_layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">prev_layer_idx</span><span class="si">}</span><span class="s2">_out, &quot;</span>
                                   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prev_size</span><span class="si">}</span><span class="s2"> * sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Flatten  GroupSum case: copy to flattened_output buffer</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Flatten layer: copy from layer_</span><span class="si">{</span><span class="n">prev_layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">prev_layer_idx</span><span class="si">}</span><span class="s2">_out to flattened_output&quot;</span><span class="p">)</span>
                        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">memcpy(flattened_output, layer_</span><span class="si">{</span><span class="n">prev_layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">prev_layer_idx</span><span class="si">}</span><span class="s2">_out, &quot;</span>
                                   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prev_size</span><span class="si">}</span><span class="s2"> * sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));&quot;</span><span class="p">)</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">layer_type</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
                <span class="n">layer_a</span><span class="p">,</span> <span class="n">layer_b</span><span class="p">,</span> <span class="n">layer_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
                <span class="n">is_final</span> <span class="o">=</span> <span class="p">(</span><span class="n">linear_layer_counter</span> <span class="o">==</span> <span class="n">linear_layer_count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">has_non_linear</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">lt</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;pool&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">lt</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">layer_info</span><span class="p">)</span>
                <span class="n">code</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_linear_layer_code</span><span class="p">(</span><span class="n">layer_a</span><span class="p">,</span> <span class="n">layer_b</span><span class="p">,</span> <span class="n">layer_op</span><span class="p">,</span> <span class="n">linear_layer_counter</span><span class="p">,</span> <span class="n">is_final</span><span class="p">,</span> <span class="n">has_non_linear</span><span class="p">))</span>
                <span class="n">linear_layer_counter</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">current_shape</span> <span class="o">=</span> <span class="n">output_shape</span>

        <span class="c1"># Handle case where we only have conv/pool layers (no linear layers)</span>
        <span class="k">if</span> <span class="n">linear_layer_count</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">layer_info</span><span class="p">:</span>
            <span class="n">final_layer_type</span><span class="p">,</span> <span class="n">final_layer_idx</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">final_size</span> <span class="o">=</span> <span class="n">layer_info</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">final_layer_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv&#39;</span><span class="p">,</span> <span class="s1">&#39;pool&#39;</span><span class="p">]:</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Copy final layer output to result&quot;</span><span class="p">)</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">memcpy(out, layer_</span><span class="si">{</span><span class="n">final_layer_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">final_layer_idx</span><span class="si">}</span><span class="s2">_out, &quot;</span>
                           <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">final_size</span><span class="si">}</span><span class="s2"> * sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">final_layer_type</span> <span class="o">==</span> <span class="s1">&#39;flatten&#39;</span><span class="p">:</span>
                <span class="c1"># Flatten is the final operation, copy from flattened_output to out</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">// Copy flattened output to result&quot;</span><span class="p">)</span>
                <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">memcpy(out, flattened_output, &quot;</span>
                           <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">final_size</span><span class="si">}</span><span class="s2"> * sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));&quot;</span><span class="p">)</span>

        <span class="n">code</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;}&quot;</span><span class="p">)</span>

        <span class="c1"># Add batch processing function if needed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
            <span class="n">code</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_generate_batch_processing_function</span><span class="p">())</span>

        <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">code</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_batch_processing_function</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate the batch processing function for GroupSum.&quot;&quot;&quot;</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_input_size</span><span class="p">()</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_size</span><span class="p">()</span>

        <span class="n">num_neurons_ll</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="n">log2_of_num_neurons_per_class_ll</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
            <span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">num_neurons_ll</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="p">[</span>
            <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">void apply_logic_net(bool const *inp, </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span><span class="si">}</span><span class="s2"> *out, size_t len) </span><span class="se">{{</span>
<span class="s2">    </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> *inp_temp = malloc(</span><span class="si">{</span><span class="n">input_size</span><span class="si">}</span><span class="s2">*sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));</span>
<span class="s2">    </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> *out_temp = malloc(</span><span class="si">{</span><span class="n">num_neurons_ll</span><span class="si">}</span><span class="s2">*sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));</span>
<span class="s2">    </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> *out_temp_o = malloc(</span><span class="si">{</span><span class="n">log2_of_num_neurons_per_class_ll</span><span class="si">}</span><span class="s2">*sizeof(</span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">));</span>

<span class="s2">    for(size_t i = 0; i &lt; len; ++i) </span><span class="se">{{</span>

<span class="s2">        // Converting the bool array into a bitpacked array</span>
<span class="s2">        for(size_t d = 0; d &lt; </span><span class="si">{</span><span class="n">input_size</span><span class="si">}</span><span class="s2">; ++d) </span><span class="se">{{</span>
<span class="s2">            </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> res = </span><span class="si">{</span><span class="n">BITS_TO_ZERO_LITERAL</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">;</span>
<span class="s2">            for(size_t b = 0; b &lt; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="si">}</span><span class="s2">; ++b) </span><span class="se">{{</span>
<span class="s2">                res &lt;&lt;= 1;</span>
<span class="s2">                res += !!(inp[i * </span><span class="si">{</span><span class="n">input_size</span><span class="si">}</span><span class="s2"> * </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="si">}</span><span class="s2"> + (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="si">}</span><span class="s2"> - b - 1) * </span><span class="si">{</span><span class="n">input_size</span><span class="si">}</span><span class="s2"> + d]);</span>
<span class="s2">            </span><span class="se">}}</span>
<span class="s2">            inp_temp[d] = res;</span>
<span class="s2">        </span><span class="se">}}</span>

<span class="s2">        // Applying the logic net</span>
<span class="s2">        logic_net(inp_temp, out_temp);</span>

<span class="s2">        // GroupSum of the results via logic gate networks</span>
<span class="s2">        for(size_t c = 0; c &lt; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="si">}</span><span class="s2">; ++c) </span><span class="se">{{</span><span class="s2">  // for each class</span>
<span class="s2">            // Initialize the output bits</span>
<span class="s2">            for(size_t d = 0; d &lt; </span><span class="si">{</span><span class="n">log2_of_num_neurons_per_class_ll</span><span class="si">}</span><span class="s2">; ++d) </span><span class="se">{{</span>
<span class="s2">                out_temp_o[d] = </span><span class="si">{</span><span class="n">BITS_TO_ZERO_LITERAL</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2">;</span>
<span class="s2">            </span><span class="se">}}</span>

<span class="s2">            // Apply the adder logic gate network</span>
<span class="s2">            for(size_t a = 0; a &lt; </span><span class="si">{</span><span class="n">num_neurons_ll</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="si">}</span><span class="s2">; ++a) </span><span class="se">{{</span>
<span class="s2">                </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> carry = out_temp[c * </span><span class="si">{</span><span class="n">num_neurons_ll</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="si">}</span><span class="s2"> + a];</span>
<span class="s2">                </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> out_temp_o_d;</span>
<span class="s2">                for(int d = </span><span class="si">{</span><span class="n">log2_of_num_neurons_per_class_ll</span><span class="si">}</span><span class="s2"> - 1; d &gt;= 0; --d) </span><span class="se">{{</span>
<span class="s2">                    out_temp_o_d  = out_temp_o[d];</span>
<span class="s2">                    out_temp_o[d] = carry ^ out_temp_o_d;</span>
<span class="s2">                    carry         = carry &amp; out_temp_o_d;</span>
<span class="s2">                </span><span class="se">}}</span>
<span class="s2">            </span><span class="se">}}</span>

<span class="s2">            // Unpack the result bits</span>
<span class="s2">            for(size_t b = 0; b &lt; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="si">}</span><span class="s2">; ++b) </span><span class="se">{{</span>
<span class="s2">                const </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> bit_mask = </span><span class="si">{</span><span class="n">BITS_TO_ONE_LITERAL</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">]</span><span class="si">}</span><span class="s2"> &lt;&lt; b;</span>
<span class="s2">                </span><span class="si">{</span><span class="n">BITS_TO_DTYPE</span><span class="p">[</span><span class="mi">32</span><span class="p">]</span><span class="si">}</span><span class="s2"> res = 0;</span>
<span class="s2">                for(size_t d = 0; d &lt; </span><span class="si">{</span><span class="n">log2_of_num_neurons_per_class_ll</span><span class="si">}</span><span class="s2">; ++d) </span><span class="se">{{</span>
<span class="s2">                    res &lt;&lt;= 1;</span>
<span class="s2">                    res += !!(out_temp_o[d] &amp; bit_mask);</span>
<span class="s2">                </span><span class="se">}}</span>
<span class="s2">                out[(i * </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="si">}</span><span class="s2"> + b) * </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="si">}</span><span class="s2"> + c] = res;</span>
<span class="s2">            </span><span class="se">}}</span>
<span class="s2">        </span><span class="se">}}</span>
<span class="s2">    </span><span class="se">}}</span>
<span class="s2">    free(inp_temp);</span>
<span class="s2">    free(out_temp);</span>
<span class="s2">    free(out_temp_o);</span>
<span class="se">}}</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_input_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the total input size.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input shape not set&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the total output size.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Get the final layer info</span>
            <span class="n">layer_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_layer_output_sizes_and_shapes</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">layer_info</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">layer_info</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span>  <span class="c1"># output_size</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No layers found&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="CompiledLogicNet.compile">
<a class="viewcode-back" href="../../api/torchlogix.html#torchlogix.CompiledLogicNet.compile">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">opt_level</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">save_lib_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compile the network to a shared library.&quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s2">&quot;.so&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">lib_file</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s2">&quot;.c&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">c_file</span><span class="p">:</span>
                <span class="n">code</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_c_code</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">verbose</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">code</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="mi">200</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">code</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">c_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>
                <span class="n">c_file</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="n">n_lines</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">code</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">))</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;C code created with </span><span class="si">{</span><span class="n">n_lines</span><span class="si">}</span><span class="s2"> lines. (temp location </span><span class="si">{</span><span class="n">c_file</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

                <span class="n">t_s</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="n">compiler_out</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cpu_compiler</span><span class="p">,</span> <span class="s2">&quot;-shared&quot;</span><span class="p">,</span> <span class="s2">&quot;-fPIC&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;-O</span><span class="si">{</span><span class="n">opt_level</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;-o&quot;</span><span class="p">,</span> <span class="n">lib_file</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">c_file</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                <span class="p">])</span>

                <span class="k">if</span> <span class="n">compiler_out</span><span class="o">.</span><span class="n">returncode</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;compilation exited with error code </span><span class="si">{</span><span class="n">compiler_out</span><span class="o">.</span><span class="n">returncode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compiling finished in </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t_s</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">save_lib_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">shutil</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">lib_file</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">save_lib_path</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;lib_file copied from </span><span class="si">{</span><span class="n">lib_file</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> to </span><span class="si">{</span><span class="n">save_lib_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="n">lib</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">cdll</span><span class="o">.</span><span class="n">LoadLibrary</span><span class="p">(</span><span class="n">lib_file</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_setup_library_function</span><span class="p">(</span><span class="n">lib</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_setup_library_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lib</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Setup the library function with appropriate signatures.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
            <span class="n">lib_fn</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">apply_logic_net</span>
            <span class="n">lib_fn</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">lib_fn</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ctypeslib</span><span class="o">.</span><span class="n">ndpointer</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">c_bool</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="s2">&quot;C_CONTIGUOUS&quot;</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ctypeslib</span><span class="o">.</span><span class="n">ndpointer</span><span class="p">(</span><span class="n">BITS_TO_C_DTYPE</span><span class="p">[</span><span class="mi">32</span><span class="p">],</span> <span class="n">flags</span><span class="o">=</span><span class="s2">&quot;C_CONTIGUOUS&quot;</span><span class="p">),</span>
                <span class="n">ctypes</span><span class="o">.</span><span class="n">c_size_t</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lib_fn</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">logic_net</span>
            <span class="n">lib_fn</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">lib_fn</span><span class="o">.</span><span class="n">argtypes</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ctypeslib</span><span class="o">.</span><span class="n">ndpointer</span><span class="p">(</span><span class="n">BITS_TO_C_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">],</span> <span class="n">flags</span><span class="o">=</span><span class="s2">&quot;C_CONTIGUOUS&quot;</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ctypeslib</span><span class="o">.</span><span class="n">ndpointer</span><span class="p">(</span><span class="n">BITS_TO_C_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">],</span> <span class="n">flags</span><span class="o">=</span><span class="s2">&quot;C_CONTIGUOUS&quot;</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lib_fn</span> <span class="o">=</span> <span class="n">lib_fn</span>

<div class="viewcode-block" id="CompiledLogicNet.forward">
<a class="viewcode-back" href="../../api/torchlogix.html#torchlogix.CompiledLogicNet.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">typing</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">]],</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through the compiled network.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_with_groupsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_direct</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_forward_with_groupsum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass with GroupSum (batch processing).&quot;&quot;&quot;</span>
        <span class="n">batch_size_div_bits</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">)</span>
        <span class="n">pad_len</span> <span class="o">=</span> <span class="n">batch_size_div_bits</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">pad_len</span><span class="p">])])</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x.shape&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">BITS_TO_NP_DTYPE</span><span class="p">[</span><span class="mi">32</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lib_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">batch_size_div_bits</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size_div_bits</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_len</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:</span><span class="o">-</span><span class="n">pad_len</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out.shape&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_forward_direct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Direct forward pass without GroupSum.&quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_input_size</span><span class="p">()</span>
        <span class="n">x_flat</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">BITS_TO_NP_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">])</span>

        <span class="n">output_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_output_size</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">BITS_TO_NP_DTYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bits</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lib_fn</span><span class="p">(</span><span class="n">x_flat</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<div class="viewcode-block" id="CompiledLogicNet.load">
<a class="viewcode-back" href="../../api/torchlogix.html#torchlogix.CompiledLogicNet.load">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="n">save_lib_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load a compiled network from a shared library.&quot;&quot;&quot;</span>
        <span class="bp">self</span> <span class="o">=</span> <span class="n">CompiledLogicNet</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_bits</span><span class="o">=</span><span class="n">num_bits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="n">lib</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">cdll</span><span class="o">.</span><span class="n">LoadLibrary</span><span class="p">(</span><span class="n">save_lib_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setup_library_function</span><span class="p">(</span><span class="n">lib</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024 Felix Petersen (original DiffLogic), Lino Gerlach (TorchLogix extensions).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>